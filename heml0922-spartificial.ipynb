{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center> Hunt for Exoplanets using Machine Learning\n\n--- \n    \n**Project ID: HEML0922**\n\n**Project Name: Hunting for Exoplanet with Machine Learning**\n    \n---","metadata":{"id":"-gwW9Ys1QG_o"}},{"cell_type":"markdown","source":"<center> <img src = \"https://c.tenor.com/3zb7e2Hb9f4AAAAC/galaxy-stars.gif\" width = 100%>","metadata":{"id":"c5BkOZJXg5tU"}},{"cell_type":"markdown","source":"#### **Let us start by [understanding exoplanets](https://docs.google.com/presentation/d/1-zjKAaiDt3gs4NkHbWsT4AzgZ-jqL6Oz/edit?usp=sharing&ouid=105423306669124151882&rtpof=true&sd=true)!**\n","metadata":{"id":"2o_ijuDVdvXH"}},{"cell_type":"markdown","source":"### In this notebook we will look into the following:-          \n**1)** <a href='#section1'>Explore the Exoplanet Dataset</a>          \n**2)** <a href='#section2'>Handling outliers</a>        \n**3)** <a href='#section3'>Understand KNN model for classification</a>   \n**4)** <a href='#section4'>Implementing KNN without Balancing the data</a>          \n**5)** <a href='#section5'>Handling imbalance data and then implementing KNN</a>         \n**6)** <a href='#section6'>Task for you</a> ","metadata":{"id":"18PA0Q4ZbVhJ"}},{"cell_type":"markdown","source":"<a id='section1'></a>","metadata":{"id":"TMTcDASYbVhL"}},{"cell_type":"markdown","source":"### Explore the Exoplanet Dataset \n","metadata":{"id":"yju-fMzzbVhM"}},{"cell_type":"markdown","source":"We will be dealing with the **[Kepler Space Telescope data](https://www.kaggle.com/datasets/keplersmachines/kepler-labelled-time-series-data)**      \n\n<img src=\"https://i.ytimg.com/vi/3yij1rJOefM/maxresdefault.jpg\" width = 500 height = 300>        \n\n**Steps to add the data in Kaggle notebook:-**           \n**1)** Click on top right arrow of the notebook      \n**2)** Click on \"Add Data\"         \n**3)** Copy the link of \"dataset\" and paste it in search      \n**4)** Click on \"+\" sign to add this dataset to the notebook","metadata":{"id":"587PWX5obVhP"}},{"cell_type":"markdown","source":"#### Importing the most needed libraries","metadata":{"id":"TCJOqNksfAIO"}},{"cell_type":"code","source":"#********************************************\nimport pandas as pd\nimport seaborn as sns\n#********************************************\nimport numpy as np \nimport matplotlib.pyplot as plt\n#********************************************\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')\n#********************************************","metadata":{"id":"kPCSBtFXbVhN","execution":{"iopub.status.busy":"2022-09-03T11:20:33.637772Z","iopub.execute_input":"2022-09-03T11:20:33.638418Z","iopub.status.idle":"2022-09-03T11:20:33.644673Z","shell.execute_reply.started":"2022-09-03T11:20:33.638391Z","shell.execute_reply":"2022-09-03T11:20:33.643445Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":"#### Getting into the data","metadata":{"id":"s_xhUBPbbVhQ"}},{"cell_type":"code","source":"# Let us begin with Train data\ntrain_df = pd.read_csv('../input/kepler-labelled-time-series-data/exoTrain.csv')\ntrain_df.head(10)","metadata":{"id":"8sCslEySbVhS","execution":{"iopub.status.busy":"2022-09-03T11:39:13.129230Z","iopub.execute_input":"2022-09-03T11:39:13.129617Z","iopub.status.idle":"2022-09-03T11:39:16.592203Z","shell.execute_reply.started":"2022-09-03T11:39:13.129591Z","shell.execute_reply":"2022-09-03T11:39:16.591047Z"},"trusted":true},"execution_count":163,"outputs":[]},{"cell_type":"code","source":"# Check the shape of train data\ntrain_df.shape","metadata":{"id":"FZ64b6UKbVhT","execution":{"iopub.status.busy":"2022-09-03T11:30:43.030537Z","iopub.execute_input":"2022-09-03T11:30:43.031171Z","iopub.status.idle":"2022-09-03T11:30:43.038014Z","shell.execute_reply.started":"2022-09-03T11:30:43.031135Z","shell.execute_reply":"2022-09-03T11:30:43.037011Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"markdown","source":"> *We can understand this data based on the transit method for detecting exoplanets.*  \n\n> *There are total of 5087 stars in this data.*\n\n> *For each star, we have 3197 flux values at different time intervals.*\n\n> *These flux values are used to plot the light curves we saw earlier to detect if a star has exoplanet(s) orbiting it.*","metadata":{"id":"svIV1JYibVhU"}},{"cell_type":"markdown","source":"#### Check for Missing Values","metadata":{"id":"u0bA_C5QbVhV"}},{"cell_type":"code","source":"# Display the rows with null values\ntrain_df[train_df.isnull().any(axis = 1)]  # axis = 1 ---> column","metadata":{"id":"oF7ID3AzbVhW","execution":{"iopub.status.busy":"2022-09-03T11:43:09.469214Z","iopub.execute_input":"2022-09-03T11:43:09.469565Z","iopub.status.idle":"2022-09-03T11:43:09.504882Z","shell.execute_reply.started":"2022-09-03T11:43:09.469540Z","shell.execute_reply":"2022-09-03T11:43:09.503581Z"},"trusted":true},"execution_count":172,"outputs":[]},{"cell_type":"markdown","source":"> *There are **no missing values**! We can also visualise it through heatmap.*","metadata":{"id":"8s9tsNwEbVhW"}},{"cell_type":"code","source":"sns.heatmap(train_df.isnull(), cmap = 'Set2', cbar = False)","metadata":{"id":"yk-bqlxqbVhX","execution":{"iopub.status.busy":"2022-09-03T11:20:36.880845Z","iopub.execute_input":"2022-09-03T11:20:36.881519Z","iopub.status.idle":"2022-09-03T11:20:54.683607Z","shell.execute_reply.started":"2022-09-03T11:20:36.881455Z","shell.execute_reply":"2022-09-03T11:20:54.682386Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"markdown","source":"> *The horizontal dashes in this plot would indicate the presence of missing values in respective column.*   \n\n> *As there aren't any of such dashes seen we can conclude that there are no missing values in this data.*","metadata":{"id":"GaDdz4jJbVhY"}},{"cell_type":"markdown","source":"#### Decoding labels in the data","metadata":{"id":"RoYRiT8PbVhZ"}},{"cell_type":"code","source":"# Check how many labels are there\ntrain_df['LABEL'].unique()","metadata":{"id":"Sido6EoHbVhZ","execution":{"iopub.status.busy":"2022-09-03T11:20:54.685026Z","iopub.execute_input":"2022-09-03T11:20:54.685359Z","iopub.status.idle":"2022-09-03T11:20:54.693111Z","shell.execute_reply.started":"2022-09-03T11:20:54.685331Z","shell.execute_reply":"2022-09-03T11:20:54.692039Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"# Extract the index for the stars labelled as 2\nidx_lab2 = list(train_df[train_df['LABEL'] == 2].index)\nprint(f\"Index list for label 2 star in the data:-\\n{idx_lab2}\\n\")","metadata":{"id":"OKZIPZIlbVha","execution":{"iopub.status.busy":"2022-09-03T11:20:54.694174Z","iopub.execute_input":"2022-09-03T11:20:54.695141Z","iopub.status.idle":"2022-09-03T11:20:54.714045Z","shell.execute_reply.started":"2022-09-03T11:20:54.695114Z","shell.execute_reply":"2022-09-03T11:20:54.712466Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"markdown","source":"> *There are total of **two classes**; one is for stars with exoplanets and the other for stars without exoplanets*\n\n> *Very few index for label 2 indicates that this class must belong to stars with exoplanets*\n\n> *We can also visualise this using countplot*","metadata":{"id":"uYP2HfT8bVhb"}},{"cell_type":"code","source":"# Visualise these values using countplot\nplt.figure(figsize = (3, 5))                                                   \nax = sns.countplot('LABEL', data = train_df, palette = 'Set2')                    \nax.bar_label(ax.containers[0])\nplt.title(\"Visualising count of classes\\n1 ~ Non Exoplanets | 2 ~ Exoplanets\\n\", \n          fontsize = 15, color = 'red', weight = 'bold')\nplt.show()","metadata":{"id":"PbQ-5DP1bVhb","execution":{"iopub.status.busy":"2022-09-03T11:20:54.715312Z","iopub.execute_input":"2022-09-03T11:20:54.715583Z","iopub.status.idle":"2022-09-03T11:20:54.845238Z","shell.execute_reply.started":"2022-09-03T11:20:54.715556Z","shell.execute_reply":"2022-09-03T11:20:54.844297Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"markdown","source":"> *There is a **huge imbalance** in the data which isn't good for KNN (explained later in this notebook).*\n\n> *We will need to balance it using some resampling technique and we will use RandomOverSampler for this data.*\n\n> *We'll do that after building the model with imbalanced dataset to compare the results!*","metadata":{"id":"Y7W0mLcSbVhc"}},{"cell_type":"markdown","source":"#### Replacing the labels\nFor ease of our model its always better to feed in the data in terms of 0 and 1         \n- Stars with Exoplanets: 2 $\\rightarrow$ 1      \n- Stars without Exoplanets: 1 $\\rightarrow$ 0","metadata":{"id":"ppqriOT7bVhc"}},{"cell_type":"code","source":"# Replacing labels \ntrain_df = train_df.replace({'LABEL' : {1:0, 2:1}})\nprint(\"Replacing labels...\")\n\n# Check the labels now\nprint(\"Done!\\n\")\nuniq_val = train_df.LABEL.unique()\nprint(f\"There are {len(uniq_val)} classes in the data:-\")\nprint(f\"{uniq_val[0]} - Stars with Exoplanets\\n{uniq_val[1]} - Stars without Exoplantes\")","metadata":{"id":"XtqiO4AObVhd","execution":{"iopub.status.busy":"2022-09-03T11:30:55.803531Z","iopub.execute_input":"2022-09-03T11:30:55.803933Z","iopub.status.idle":"2022-09-03T11:30:55.835231Z","shell.execute_reply.started":"2022-09-03T11:30:55.803907Z","shell.execute_reply":"2022-09-03T11:30:55.834160Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"markdown","source":"#### Visualising the light curves in this data\nWhen a planet passes between an observer and the star, the flux value decreases and hence we see a dip in light curves with exoplanets\n","metadata":{"id":"JsYssnqibVhd"}},{"cell_type":"markdown","source":"<img src = \"https://media3.giphy.com/media/495uiXmwMRnWvXU9gm/giphy.gif\" width = 100%>","metadata":{"id":"sKRogUpAbVhe"}},{"cell_type":"code","source":"# Drop label column to plot only the flux values\nplot_df = train_df.drop(['LABEL'], axis = 1)\n\n# X - axis data: Replace FLUX. from each column names\ncol_names = list(plot_df.columns)\ntime = [int(flux_prefix.replace(\"FLUX.\", \"\")) for flux_prefix in col_names]\n\n# Function to plot flux variation of star\ndef flux_plot(df, candidate, exo = True):\n    color = 'b' if exo == True else 'm'\n    plt.figure(figsize=(15, 5))\n    plt.plot(time, df.iloc[candidate-1], linewidth = .5, color = color)\n    title1, clr1 = f\"Flux Variation of star {candidate} with Exoplanents\", 'olive'\n    title2, clr2 = f\"Flux Variation of star {candidate} without Exoplanets\", 'tab:red'\n    plt.title(title1, color = clr1) if exo == True else plt.title(title2, color = clr2)\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Flux Variation\")","metadata":{"id":"FAP0AP23bVhe","execution":{"iopub.status.busy":"2022-09-03T11:31:10.514436Z","iopub.execute_input":"2022-09-03T11:31:10.514774Z","iopub.status.idle":"2022-09-03T11:31:10.548619Z","shell.execute_reply.started":"2022-09-03T11:31:10.514749Z","shell.execute_reply":"2022-09-03T11:31:10.546748Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"# Example of light curves\nexo, n_exo = [4, 14, 34], [99, 199, 2999]\n\nfor candidate in range(len(exo)):\n    flux_plot(plot_df, exo[candidate], exo = True)\n    flux_plot(plot_df, n_exo[candidate], exo = False)","metadata":{"id":"1cnuHg8ebVhf","execution":{"iopub.status.busy":"2022-09-03T11:31:15.998729Z","iopub.execute_input":"2022-09-03T11:31:15.999664Z","iopub.status.idle":"2022-09-03T11:31:17.051226Z","shell.execute_reply.started":"2022-09-03T11:31:15.999636Z","shell.execute_reply":"2022-09-03T11:31:17.050051Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section2\"></a>","metadata":{"id":"yB_vuGpKbVhf"}},{"cell_type":"markdown","source":"### Extreme outliers \n- We can see random **huge spikes** especially in stars without exoplanets which can be considered as extreme outliers\n\n- KNN can be sensitive to outliers (explained later in this notebook) so we will need to handle it \n\n- We can also visualise these extreme outliers through boxplot","metadata":{"id":"4KL3kpy3bVhf"}},{"cell_type":"code","source":"# Boxplot to visualise outliers\nplt.figure(figsize = (20, 9))\nplt.suptitle(\"Box Plot to visualise outliers\", ha = 'right', color = 'red', weight = 'bold')\nfor i in range(1, 4):\n    plt.subplot(1, 4, i)\n    sns.boxplot(data=train_df, x='LABEL', y = 'FLUX.' + str(i))\n    plt.xlabel(\"\")\n    plt.ylabel(\"\")\n    plt.title(\"FLUX \" + str(i) + \"\\n\", color = 'b', fontsize = 13)\n","metadata":{"id":"xVTFFfj4bVhg","execution":{"iopub.status.busy":"2022-09-03T11:33:49.038561Z","iopub.execute_input":"2022-09-03T11:33:49.038902Z","iopub.status.idle":"2022-09-03T11:33:49.419792Z","shell.execute_reply.started":"2022-09-03T11:33:49.038877Z","shell.execute_reply":"2022-09-03T11:33:49.418647Z"},"trusted":true},"execution_count":142,"outputs":[]},{"cell_type":"markdown","source":"> *We can see that the flux values more than $0.25 x 10^6$ are extreme outliers.*        \n\n> *We can either drop it or replace its value with upper bridge value. For this usecase, we will simply drop it.*     \n\n> *However you can try to compute on your own the upper bridge value using the formula given below:-*   \n\n> $UB = Q3 + 3 \\times IQR$; **UB** - upper bridge, **Q3** - 75th percentile, **IQR** - Interquartile range","metadata":{"id":"55GjzgeWbVhh"}},{"cell_type":"code","source":"# Get the extreme outliers\nextreme_outliers = train_df[train_df['FLUX.2'] > 0.25e6]\nextreme_outliers","metadata":{"execution":{"iopub.status.busy":"2022-09-03T11:39:46.635387Z","iopub.execute_input":"2022-09-03T11:39:46.636162Z","iopub.status.idle":"2022-09-03T11:39:46.664437Z","shell.execute_reply.started":"2022-09-03T11:39:46.636088Z","shell.execute_reply":"2022-09-03T11:39:46.663628Z"},"trusted":true},"execution_count":167,"outputs":[]},{"cell_type":"code","source":"# Drop the extreme outlier\nprint(\"Droping Extreme Outliers...\")\ntrain_df.drop(extreme_outliers.index, axis = 0, inplace = True)  # axis = 0 ----> row\nprint(\"Done!\")","metadata":{"id":"ObjdQ4wsbVhh","execution":{"iopub.status.busy":"2022-09-03T11:40:01.711204Z","iopub.execute_input":"2022-09-03T11:40:01.711589Z","iopub.status.idle":"2022-09-03T11:40:01.782348Z","shell.execute_reply.started":"2022-09-03T11:40:01.711563Z","shell.execute_reply":"2022-09-03T11:40:01.780888Z"},"trusted":true},"execution_count":168,"outputs":[]},{"cell_type":"markdown","source":"> *We have dropped the star at location 3340 which contributed to extreme outliers, let us visualise the box plots again*","metadata":{}},{"cell_type":"code","source":"# Cross check via any random box plot\nsns.boxplot(data=train_df, x='LABEL', y = 'FLUX.' + str(np.random.randint(1000)))","metadata":{"id":"hkWLtVEdbVhh","execution":{"iopub.status.busy":"2022-09-03T11:44:14.069224Z","iopub.execute_input":"2022-09-03T11:44:14.069561Z","iopub.status.idle":"2022-09-03T11:44:14.228490Z","shell.execute_reply.started":"2022-09-03T11:44:14.069538Z","shell.execute_reply":"2022-09-03T11:44:14.227697Z"},"trusted":true},"execution_count":173,"outputs":[]},{"cell_type":"markdown","source":"<a id = 'section3'></a>","metadata":{"id":"cWWnnVFVbVhi"}},{"cell_type":"markdown","source":"### Understanding K - Nearest Neighbors (KNN) Algorithm for Classification Tasks\n\n#### Why choose KNN for this task?\n- In this dataset we saw that there were outliers + it was imbalanced       \n- KNN is **sensitive** to both **outliers and imbalnced data** which you will understand in a while\n- Hence, we choose this model to demonstrate how to handle outliers and imbalanced dataset\n- Moreover KNN is one of the simplest ML algorithms based on Supervised Learning technique\n- KNN are widely used for classification (binary and multiclass classification) as compared to regression tasks\n\n#### How does KNN work?     \n\n**Steps:-**      \n**1)** Select the **number of K neighbors** for the new data point      \n**2)** Calculate the <a href = '#euclidean'>**Euclidean distance**</a> from this point to the other points in the data     \n**3)** Take the **K nearest neighbors** as per the calculated Euclidean distance     \n**4)** Among these neighbors, **count** the number of data points of each category       \n**5)** The new data point belongs to the **cateogory with maximum data points available**   \n\n*Here is one demonstration to understand how this algorith works:-*\n\n<img src = \"https://machinelearningknowledge.ai/wp-content/uploads/2018/08/KNN-Classification.gif\" width = 500 height = 300>\n\n<a id='euclidean'></a>\n\n\n**Euclidean Distance** *between two points is simply calculated using the distance between two points formula in a cartesian coordinate system:-*      \n\n<img src = \"https://hlab.stanford.edu/brian/making7.gif\" width = 400 height = 300>\n\n#### How to select value for K in this algorithm      \n- We need to try out some values of K and figure out which one is working best out of them      \n- Usually the preferred value of K is 5      \n- A very low value of K (K = 1, K = 2) can lead to effects of outliers in the model\n- A very high value of K can lead to the biasness towards imbalanced dataset\n\n<img src = \"https://drive.google.com/uc?id=1t1l1vqxIn9sNwyFWbI4NVxyeXC_-GQVu\" width = 400 height = 300>  <img src = \"https://drive.google.com/uc?id=1vmctmaafTVxa4mVt6raExJ-fEL3VBdde\" width = 400 height = 300>\n\n- If the value of K isn't very low and dataset is balanced than the outliers would not really effect our model\n- Lets take K = 150 in that case you can see the imbalance in the data can lead to a very poor result! \n\n\n#### Advantages of KNN\n**1)** One of the simplest to understand and implement        \n**2)** Depending on value of K it can be robust to the noisy training data         \n**3)** It can be more effective if training data is large          \n\n#### Disadvantages of KNN         \n**1)** Sometimes detrming the value of K can become a complex task           \n**2)** High computation cost as we are calculating the distances between the data points for all the training data","metadata":{"id":"EVgrEYyvbVhi"}},{"cell_type":"markdown","source":"<a id='section4'></a>","metadata":{"id":"zxCHlY6WbVhi"}},{"cell_type":"markdown","source":"### Implementing KNN after handling the extreme outliers but have yet not balanced the data\n*It would be interesting to compare the results with and without imbalance in our data. Let us first start with imbalanced data:-*","metadata":{"id":"ewVyNHhebVhj"}},{"cell_type":"code","source":"# Extract dependent and independent features\nx = train_df.drop(['LABEL'], axis = 1)\ny = train_df.LABEL\n\nprint(f\"Take a look over ~\\n\\nX train array:-\\n{x.values}\\n\\nY train array:-\\n{y.values}\")","metadata":{"id":"cynthuX8bVhj","execution":{"iopub.status.busy":"2022-09-03T11:31:53.032461Z","iopub.execute_input":"2022-09-03T11:31:53.032821Z","iopub.status.idle":"2022-09-03T11:31:53.064829Z","shell.execute_reply.started":"2022-09-03T11:31:53.032796Z","shell.execute_reply":"2022-09-03T11:31:53.063371Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"# Splitting this dataset into training and testing set\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 0)","metadata":{"id":"h9xwQvdsbVhk","execution":{"iopub.status.busy":"2022-09-03T11:20:56.695330Z","iopub.execute_input":"2022-09-03T11:20:56.696390Z","iopub.status.idle":"2022-09-03T11:20:56.792324Z","shell.execute_reply.started":"2022-09-03T11:20:56.696363Z","shell.execute_reply":"2022-09-03T11:20:56.791523Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"# Feature scaling\nfrom sklearn.preprocessing import StandardScaler \n\nsc = StandardScaler()\nX_train_sc = sc.fit_transform(X_train)\nX_test_sc = sc.transform(X_test)\n\n# Checking the minimum, mean and maxmum value after scaling\nprint(\"X_train after scaling ~\\n\")\nprint(f\"Minimum:- {round(np.min(X_train_sc),2)}\\nMean:- {round(np.mean(X_train_sc),2)}\\nMax:- {round(np.max(X_train_sc), 2)}\\n\")\nprint(\"--------------------------------\\n\")\nprint(\"X_test after scaling ~\\n\")\nprint(f\"Minimum:- {round(np.min(X_test_sc),2)}\\nMean:- {round(np.mean(X_test_sc),2)}\\nMax:- {round(np.max(X_test_sc), 2)}\\n\")","metadata":{"id":"VagRDCuebVhk","execution":{"iopub.status.busy":"2022-09-03T11:20:56.793203Z","iopub.execute_input":"2022-09-03T11:20:56.793411Z","iopub.status.idle":"2022-09-03T11:20:57.012448Z","shell.execute_reply.started":"2022-09-03T11:20:56.793390Z","shell.execute_reply":"2022-09-03T11:20:57.011266Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"# Fiting the KNN Classifier Model on to the training data\nfrom sklearn.neighbors import KNeighborsClassifier as KNC\n\n# Choosing K = 10\nknn_classifier = KNC(n_neighbors=5,metric='minkowski',p=2)  \n'''metric is to be by default minkowski for p = 2 to calculate the Eucledian distances'''\n\n# Fit the model\nknn_classifier.fit(X_train_sc, y_train)\n\n# Predict\ny_pred = knn_classifier.predict(X_test_sc)\n\n# Results\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_curve, auc\n\nprint('\\nValidation accuracy of KNN is', accuracy_score(y_test,y_pred))\nprint(\"\\n-------------------------------------------------------\")\nprint (\"\\nClassification report :\\n\",(classification_report(y_test,y_pred)))\n\n#Confusion matrix\nplt.figure(figsize=(15,11))\nplt.subplots_adjust(wspace = 0.3)\nplt.suptitle(\"KNN Performance before handling the imbalance in the data\", color = 'r', weight = 'bold')\nplt.subplot(221)\nsns.heatmap(confusion_matrix(y_test,y_pred),annot=True,cmap=\"Set2\",fmt = \"d\",linewidths=3, cbar = False,\n           xticklabels=['nexo', 'exo'], yticklabels=['nexo','exo'], square = True)\nplt.xlabel(\"True Labels\", fontsize = 15, weight = 'bold', color = 'tab:pink')\nplt.ylabel(\"Predicited Labels\", fontsize = 15, weight = 'bold', color = 'tab:pink')\nplt.title(\"CONFUSION MATRIX\",fontsize=20, color = 'm')\n\n#ROC curve and Area under the curve plotting\npredicting_probabilites = knn_classifier.predict_proba(X_test_sc)[:,1]\nfpr,tpr,thresholds = roc_curve(y_test,predicting_probabilites)\nplt.subplot(222)\nplt.plot(fpr,tpr,label = (\"AUC :\",auc(fpr,tpr)),color = \"g\")\nplt.plot([1,0],[1,0],\"k--\")\nplt.legend()\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC - CURVE & AREA UNDER CURVE\",fontsize=20, color = 'm')\nplt.show()\n\n","metadata":{"id":"8I4vQE0wbVhk","execution":{"iopub.status.busy":"2022-09-03T11:20:57.015249Z","iopub.execute_input":"2022-09-03T11:20:57.015529Z","iopub.status.idle":"2022-09-03T11:20:58.403761Z","shell.execute_reply.started":"2022-09-03T11:20:57.015505Z","shell.execute_reply":"2022-09-03T11:20:58.401413Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"markdown","source":"> *Even though the accuracy is amazing, this isn't really a good model. This is due to the huge imbalance in the dataset!*      \n\n> *We need to check for other metrics like **precission**, **recall**, **f1 score** in such models*       \n\n<center> <img src = \"https://www.researchgate.net/profile/B-Aksasse/publication/326866871/figure/fig3/AS:669601385959430@1536656819610/22-confusion-matrix-and-associated-measures.ppm\" width = 700 height = 300>               \n    \n    \n> *Beliving this as a very good model based on only the accuracy can really perform bad on an unseen data*","metadata":{"id":"4V3OC_7vbVhl"}},{"cell_type":"markdown","source":"### Handling the imbalance in the data and then applying KNN\n*There are many techniques available out of which we will be trying* ***RandomOverSampler***:-     \n RandomOverSampler is over-sampling by duplicating some of the original samples of the minority class","metadata":{"id":"n1oK_CTWbVhm"}},{"cell_type":"markdown","source":"<a id='section5'></a>","metadata":{"id":"luXeGVQobVhm"}},{"cell_type":"code","source":"# Handling imbalanced data using RandomOverSampler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom collections import Counter\n\nros = RandomOverSampler()\nx_ros, y_ros = ros.fit_resample(x, y)  # Taking the original x, y as arguments\n\nprint(f\"Before sampling:- {Counter(y)}\")\nprint(f\"After sampling:- {Counter(y_ros)}\")","metadata":{"id":"5HFfM7SbbVhm","execution":{"iopub.status.busy":"2022-09-03T11:44:41.308648Z","iopub.execute_input":"2022-09-03T11:44:41.309210Z","iopub.status.idle":"2022-09-03T11:44:42.038079Z","shell.execute_reply.started":"2022-09-03T11:44:41.309182Z","shell.execute_reply":"2022-09-03T11:44:42.037006Z"},"trusted":true},"execution_count":175,"outputs":[]},{"cell_type":"code","source":"# Visualise it\ny_ros.value_counts().plot(kind='bar', title='After aplying RandomOverSampler');\n","metadata":{"id":"V0JzAejgbVhn","execution":{"iopub.status.busy":"2022-09-03T12:39:34.594791Z","iopub.execute_input":"2022-09-03T12:39:34.595283Z","iopub.status.idle":"2022-09-03T12:39:35.143271Z","shell.execute_reply.started":"2022-09-03T12:39:34.595258Z","shell.execute_reply":"2022-09-03T12:39:35.142107Z"},"trusted":true},"execution_count":216,"outputs":[]},{"cell_type":"markdown","source":"#### Repeating the above steps","metadata":{"execution":{"iopub.status.busy":"2022-09-03T13:33:51.118133Z","iopub.execute_input":"2022-09-03T13:33:51.118507Z","iopub.status.idle":"2022-09-03T13:33:51.126437Z","shell.execute_reply.started":"2022-09-03T13:33:51.118461Z","shell.execute_reply":"2022-09-03T13:33:51.125012Z"}}},{"cell_type":"code","source":"#  ****************************************************************\n# | Performing split and scaling on the random over sampled data  |\n# ****************************************************************\n\nX_train, X_test, y_train, y_test = train_test_split(x_ros, y_ros, test_size = 0.3, random_state = 0)\n\nsc = StandardScaler()\nX_train_sc = sc.fit_transform(X_train)\nX_test_sc = sc.transform(X_test)","metadata":{"id":"kgiRkAU4bVhn","execution":{"iopub.status.busy":"2022-09-03T11:20:59.632609Z","iopub.execute_input":"2022-09-03T11:20:59.633031Z","iopub.status.idle":"2022-09-03T11:21:00.265216Z","shell.execute_reply.started":"2022-09-03T11:20:59.632994Z","shell.execute_reply":"2022-09-03T11:21:00.263695Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"markdown","source":"##### Creating a function to try to fetch the optimal value of K","metadata":{}},{"cell_type":"code","source":"# Create function to fetch the optimal value of K\ndef optimal_Kval_KNN(start_k, end_k, x_train, x_test, y_train, y_test, progress = True):\n    ''' \n    This function takes in the following arguments -\n    start_k - start value of k\n    end_k - end value of k\n    x_train - independent training values for training the KNN\n    x_test - independent testing values for prediction\n    y_train - dependent training values for training KNN\n    y_test - dependent testing values for computing error rate\n    progress - if true shows the progress for each k (by default its set to True)\n    '''\n    # Header\n    print(f\"Fetching the optimal value of K in between {start_k} & {end_k} ~\\n\\nIn progress...\")\n    \n    # Empty list to append error rate\n    mean_err = []\n    for K in range(start_k, end_k + 1):                         # Generates K from start to end-1 values\n        knn = KNC(n_neighbors = K)                              # Build KNN for respective K value\n        knn.fit(x_train, y_train)                               # Train the model\n        err_rate = np.mean(knn.predict(x_test) != y_test)       # Get the error rate\n        mean_err.append(err_rate)                               # Append it\n        # If progress is true display the error rate for each K\n        if progress == True:print(f'For K = {K}, mean error = {err_rate:.3}')\n        \n    # Get the optimal value of k and corresponding value of mean error\n    k, val = mean_err.index(min(mean_err))+1, min(mean_err)\n    \n    # Footer\n    print('\\nDone! Here is how error rate varies wrt to K values:- \\n')\n    \n    # Display how error rate changes wrt K values and mark the optimal K value\n    plt.figure(figsize = (5,5))\n    plt.plot(range(start_k,end_k + 1), mean_err, 'mo--', markersize = 8, markerfacecolor = 'c',\n            linewidth = 1)          # plots all mean error wrt K values\n    plt.plot(k, val, marker = 'o', markersize = 8, markerfacecolor = 'gold', \n             markeredgecolor = 'g') # highlits the optimal K\n    plt.title(f\"The optimal performance is obtained at K = {k}\", color = 'r', weight = 'bold',\n             fontsize = 15)\n    plt.ylabel(\"Error Rate\", color = 'olive', fontsize = 13)\n    plt.xlabel(\"K values\", color = 'olive', fontsize = 13)\n    \n    '''returns the optimal value of k'''\n    return k","metadata":{"id":"ndmM1A10bVhn","execution":{"iopub.status.busy":"2022-09-03T11:21:00.266658Z","iopub.execute_input":"2022-09-03T11:21:00.266914Z","iopub.status.idle":"2022-09-03T11:21:00.278081Z","shell.execute_reply.started":"2022-09-03T11:21:00.266891Z","shell.execute_reply":"2022-09-03T11:21:00.277245Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"k = optimal_Kval_KNN(1, 10, X_train_sc, X_test_sc, y_train, y_test)","metadata":{"id":"Ju3OoHr0uMLh","execution":{"iopub.status.busy":"2022-09-03T11:21:00.279335Z","iopub.execute_input":"2022-09-03T11:21:00.279642Z","iopub.status.idle":"2022-09-03T11:21:21.693217Z","shell.execute_reply.started":"2022-09-03T11:21:00.279615Z","shell.execute_reply":"2022-09-03T11:21:21.692435Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"markdown","source":"> *Seems like **K = 1** shall do the job! Let's try*","metadata":{"id":"iRPzLecDbVho"}},{"cell_type":"code","source":"# Fiting the KNN Classifier Model on to the training data after\n\n# Choosing K = 1\nknn_classifier = KNC(n_neighbors=1,metric='minkowski',p=2)  \n'''metric is to be by default minkowski for p = 2 to calculate the Eucledian distances'''\n\n# Fit the model\nknn_classifier.fit(X_train_sc, y_train)\n\n# Predict\ny_pred = knn_classifier.predict(X_test_sc)\n\n# Results\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_curve, auc\n\nprint('\\nValidation accuracy of KNN is', accuracy_score(y_test,y_pred))\nprint(\"\\n-------------------------------------------------------\")\nprint (\"\\nClassification report :\\n\",(classification_report(y_test,y_pred)))\n\n#Confusion matrix\nplt.figure(figsize=(15,11))\nplt.subplots_adjust(wspace = 0.3)\nplt.suptitle(\"KNN Performance after handling the imbalance in the data\", color = 'b', weight = 'bold')\nplt.subplot(221)\nsns.heatmap(confusion_matrix(y_test,y_pred),annot=True,cmap=\"Set2\",fmt = \"d\",linewidths=3, cbar = False,\n           xticklabels=['nexo', 'exo'], yticklabels=['nexo','exo'], square = True)\nplt.xlabel(\"True Labels\", fontsize = 15, weight = 'bold', color = 'm')\nplt.ylabel(\"Predicited Labels\", fontsize = 15, weight = 'bold', color = 'm')\nplt.title(\"CONFUSION MATRIX\",fontsize=20, color = 'purple')\n\n#ROC curve and Area under the curve plotting\npredicting_probabilites = knn_classifier.predict_proba(X_test_sc)[:,1]\nfpr,tpr,thresholds = roc_curve(y_test,predicting_probabilites)\nplt.subplot(222)\nplt.plot(fpr,tpr,label = (\"AUC :\",auc(fpr,tpr)),color = \"g\")\nplt.plot([1,0],[1,0], 'k--')\nplt.legend(loc = \"best\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC - CURVE & AREA UNDER CURVE\",fontsize=20, color = 'm')\nplt.show()","metadata":{"id":"mLjStmN1bVho","execution":{"iopub.status.busy":"2022-09-03T14:12:09.753128Z","iopub.execute_input":"2022-09-03T14:12:09.753498Z","iopub.status.idle":"2022-09-03T14:12:13.719203Z","shell.execute_reply.started":"2022-09-03T14:12:09.753445Z","shell.execute_reply":"2022-09-03T14:12:13.717714Z"},"trusted":true},"execution_count":260,"outputs":[]},{"cell_type":"markdown","source":"> *We can see now all the metrics we talked about earlier is showing good results on the splitted testing set!*","metadata":{"id":"STy7R4g_bVho"}},{"cell_type":"markdown","source":"<a id='section6'></a>","metadata":{"id":"OXKBAceNbVhp"}},{"cell_type":"markdown","source":"### Task for you\n\n- Is this model working well for unseen data (test set)? Try it yourself! \n- If **yes**,\n    - *Can you name any other models that would work better than KNN?*\n    - *Try building a model that would work better than what you get after testing the unseen data*\n    - [*Submit*](https://forms.gle/gVW3Spv148dMzamo9) *your notebook by completing these tasks and get certified for solving this problem*\n-If **not**, \n    - *What could be the reason we might have overlooked while building our model?*\n    - *Can you come up with a better model (not necessary to use KNN) that will work well with the test set?* \n    - [*Submit*](https://forms.gle/gVW3Spv148dMzamo9) *your notebook by completing these tasks and get certified for solving this problem*\n","metadata":{"id":"nWsq06aJbVhp"}},{"cell_type":"markdown","source":"---\n\n# <center> THE END","metadata":{"id":"StZ16ktLhn_I"}}]}